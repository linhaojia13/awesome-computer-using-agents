# awesome-computer-using-agents

1. [Benchmark](#1-benchmark)
2. [Architecture](#2-architecture)
3. [Data Synth](#3-data-synth)
4. [Reward Model](#4-reward-model)
5. [Training Algorithm](#5-training-algorithm)
6. [Others](#6-others)

## 1. Benchmark
[2508] [OSWorld-Verified](https://xlang.ai/blog/osworld-verified)
> ä¿®æ­£äº†OSWorldçš„ä¸€äº›bugï¼šé—®é¢˜æ¨¡ç³Šã€ç½‘ç«™æ‰“ä¸å¼€ï¼›  
> æŒ‡å‡ºç¤¾åŒºä¸‹ä¸€æ­¥é‡ç‚¹æ˜¯æ„é€ è½¨è¿¹æ•°æ®ã€éªŒè¯å™¨ã€æ›´æœ‰æ•ˆçš„æµ‹è¯„  
> **å¼€æºäº†[ç°æœ‰çš„CUAåœ¨osworldæµ‹è¯•é›†ä¸Šçš„æµ‹è¯„è½¨è¿¹](https://huggingface.co/datasets/xlangai/ubuntu_osworld_verified_trajs)ï¼ŒæŒºæœ‰å‚è€ƒä»·å€¼çš„**

ğŸŒŸ [2506] [OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents](https://arxiv.org/pdf/2506.16042)
> äººç±»30ç§’å®Œæˆçš„ä»»åŠ¡ï¼ˆå¦‚è°ƒæ•´æ–‡æ¡£è¡Œè·ï¼‰ï¼Œä»£ç†éœ€è€—æ—¶12åˆ†é’Ÿ  
> äººå·¥æ ‡æ³¨OSWorldçš„369ä¸ªçœŸå®ä»»åŠ¡ï¼Œæ„å»ºäººç±»æ“ä½œè½¨è¿¹ï¼Œå‘ç°äººç±»æ“ä½œæ­¥éª¤æ¯”AIä»£ç†å°‘1.4â€“2.7å€  
> æå‡ºæ–°è¯„ä¼°æŒ‡æ ‡ï¼šWESï¼ˆåŠ æƒæ•ˆç‡å¾—åˆ†ï¼‰

[2504] [Computer Agent Arena: Compare & Test AI Agents on Crowdsourced Real-World Computer Use Tasks](https://arena.xlang.ai)
> é’ˆå¯¹CUAçš„ç«æŠ€åœº  
> åšå®¢é‡Œæåˆ°ä¼šé€šè¿‡è¿™ä¸ªarena**æ”¶é›†ç”¨æˆ·åå¥½çš„æŒ‡ä»¤æ•°æ®**ï¼Œå¹¶åœ¨æœªæ¥å¼€æº

[2404] [OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments](https://os-world.github.io)
> 369ä¸ªtaskï¼Œæ¯ä¸ªtaskå¯¹åº”ä¸€ä¸ªè„šæœ¬ä½œä¸ºéªŒè¯å™¨ 

## 2. Architecture
ğŸŒŸ [2508] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/pdf/2508.03923)
> ä¸‰å¤§æ™ºèƒ½ä½“åˆ†å·¥â€‹: 1) Orchestratorâ€‹ï¼šä»»åŠ¡åˆ†è§£ä¸å†³ç­–ä¸­æ¢; 2) Programmerâ€‹ï¼šé€šè¿‡ä»£ç ä¸ç³»ç»Ÿç›´æ¥äº¤äº’; 3) GUI Operatorâ€‹ï¼šåŸºäºè§†è§‰æ¨¡å‹æ‰§è¡Œå›¾å½¢æ“ä½œ  
> CoAct-1 vs GTA-1: æˆåŠŸç‡60.76 vs 45.2ï¼Œå¹³å‡æ­¥æ•°10.15 vs 15.22

[2507] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/pdf/2507.05791)
> planner + judge + grouder  
> plannerè¾“å‡ºå¤šä¸ªcandidate actionï¼Œjudgeä»ä¸­é€‰æ‹©actionï¼Œgrouderè½¬æ¢ä¸ºå¸¦åæ ‡çš„åŠ¨ä½œ
> rlè®­ç»ƒgrouderï¼Œo3ä½œä¸ºplanner+judgeï¼Œosworldèƒ½è¾¾åˆ°45%

ğŸŒŸ [2505][Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://osworld-grounding.github.io)
> ç”¨JEDI-7Bä½œä¸ºgroudingæ¨¡å‹ï¼Œé…åˆGPT-4oè§„åˆ’å™¨ï¼Œåœ¨OSWorldåŸºå‡†ä¸Šçš„æˆåŠŸç‡é«˜è¾¾27%  
> jedi-7b-o3åœ¨OSWorldä¸Šåˆ™æ˜¯é«˜è¾¾50
> è®­ç»ƒåçš„æ¨¡å‹ï¼ˆå¦‚JEDI-7Bï¼‰â€‹ä¸ç›´æ¥è¾“å‡ºPyAutoGUIä»£ç ï¼Œè€Œæ˜¯é¢„æµ‹ç»“æ„åŒ–åŠ¨ä½œï¼ˆ38é¡µé™„å½•A.4ï¼‰

[2504][Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents](https://arxiv.org/pdf/2504.00906)


## 3. Data Synth
### 3.1 Computer
ğŸŒŸ [2509][UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning](https://arxiv.org/pdf/2509.02544)


ğŸŒŸ [2508][Mobile-Agent-v3: Foundamental Agents for GUI Automation](https://arxiv.org/pdf/2508.15144)
> é¢„è®­ç»ƒ+sftå¾—åˆ°gui-owl-7båœ¨osworldä¸Š29.4ï¼›åœ¨osworldä¸Šä¸“é—¨rlåæ˜¯34.9  


[2508][ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/pdf/2508.14040)
> We manually collect extensive, well-defined tasks and corresponding evaluation functions. é€šè¿‡äººå·¥æ„å»ºä»»åŠ¡ã€éªŒè¯å‡½æ•°  
> If the trajectory successfully solves the task, we assign a reward of 1 to every action that is both correctly formatted and substantially contributes to the solution. éªŒè¯å‡½æ•°åº”è¯¥æ˜¯å¤æ‚åˆ°æ¯ä¸ªactionæ˜¯å¦å¯¹ä»»åŠ¡å®Œæˆæœ‰ç›Šã€‚


[2508][SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/pdf/2508.04037)
> 7bæ¨¡å‹åœ¨osworldä¸Š30.1ï¼Œæ¯”SE-Agentã€OpenCUAéƒ½æ›´å¼º  
> åŸºäºQWen2.5-VL-72Bè®­ç»ƒå‡ºæ¥çš„step modelå¯ä»¥åˆ¤æ–­actionçš„æ­£ç¡®æ€§ï¼Œä½†æ²¡å±•ç¤ºprompt  
> é€šè¿‡few-shot in-context Learningå»ç”Ÿäº§task

ğŸŒŸ [2508] [SEAgent: Self-Evolving Computer Use Agent with
Autonomous Learning from Experience](https://arxiv.org/pdf/2508.04700)
> Curriculum Generator: ç”Ÿæˆä»»åŠ¡ï¼Œæäº†4245ä¸ªä»»åŠ¡  
> World State Model: ä¹Ÿå°±æ˜¯reward model, ç”¨çš„æ˜¯åŸºäºosworld chomeçš„43ä¸ªtaskå¾—åˆ°çš„860æ¡è½¨è¿¹è®­ç»ƒqwen2.5vl-7bå¾—åˆ°çš„

[2508] [VeriGUI: Verifiable Long-Chain GUI Dataset](https://arxiv.org/pdf/2508.04026)
> VeriGUI æ„å»ºäº†é¦–ä¸ªæ”¯æŒ â€‹é•¿é“¾å¤æ‚ä»»åŠ¡â€‹ï¼ˆæ•°ç™¾æ­¥æ“ä½œï¼‰å’Œ â€‹å­ä»»åŠ¡çº§éªŒè¯â€‹ çš„ GUI æ•°æ®é›†ï¼Œè¦†ç›–æ¡Œé¢å’Œç½‘é¡µç¯å¢ƒã€‚  
> æ•°æ®é›†æŒç»­æ›´æ–°ä¸­ï¼Œç›®å‰åªæœ‰130ç½‘é¡µä»»åŠ¡ï¼Œæ¡Œé¢ä»»åŠ¡å³å°†å‘å¸ƒã€‚  
> æ¡Œé¢éªŒè¯æœºåˆ¶åŸºäºæˆªå›¾å’Œç³»ç»Ÿå±æ€§çš„çŠ¶æ€æ£€æŸ¥ï¼Œç”±äººç±»ä¸“å®¶åœ¨æ ‡æ³¨è¿‡ç¨‹ä¸­å®ç°å‡½æ•°åŒ–

ğŸŒŸ [2508] [OpenCUA: Open Foundations for Computer-Use Agents](https://opencua.xlang.ai)
> 634åæ ‡æ³¨å‘˜é’ˆå¯¹200+åº”ç”¨/ç½‘ç«™ï¼Œæ ‡è®°äº†20k+ä»»åŠ¡çš„è½¨è¿¹  
> å¼€æºäº†æ ‡æ³¨è½¯ä»¶ã€æ•°æ®é›†  
> çº¯sftè®­ç»ƒqwen-vl-32bï¼Œosworldä¸Šåˆ†æ•°34.8

[2506] [AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents](https://arxiv.org/pdf/2506.14205)
> åˆ©ç”¨ä¿¡æ¯ä¸å¯¹ç§°ï¼Œé“¾å¼ç”Ÿæˆå­ä»»åŠ¡â€‹ï¼Œç»„åˆæˆå¤æ‚ä»»åŠ¡ã€‚ä»»åŠ¡ç”ŸæˆæˆåŠŸç‡98%ï¼Œä½†è¯„ä¼°æ—¶SOTAæ¨¡å‹åœ¨Level 6ä»»åŠ¡æˆåŠŸç‡ä»…4%â€‹â€‹  
> å¯æ§ä»»åŠ¡éš¾åº¦: é€šè¿‡å­ä»»åŠ¡æ•°é‡ç²¾ç¡®è°ƒæ§å¤æ‚åº¦ï¼ŒLevel 6ä»»åŠ¡å¹³å‡éœ€40â€“60æ­¥æ“ä½œ  
> â€‹é«˜çœŸå®æ€§ä»»åŠ¡: äººå·¥è¯„ä¼°æ˜¾ç¤ºï¼šä»»åŠ¡å¯è¡Œæ€§87%â€‹ã€å­ä»»åŠ¡è¿è´¯æ€§91%â€‹ã€è§’è‰²ç›¸å…³æ€§94%â€‹  
> æ­ç¤ºLLMä»£ç†ä¸‰å¤§ç¼ºé™·ï¼šé¼ æ ‡ç‚¹å‡»ä¸ç²¾å‡†ï¼ˆ59.1%åŠ¨ä½œï¼‰ã€çŠ¶æ€è·Ÿè¸ªé”™è¯¯ã€é”™è¯¯æ¢å¤èƒ½åŠ›ç¼ºå¤±ï¼ˆç¬¬5.3èŠ‚ï¼‰

[2505] [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://osworld-grounding.github.io)
> å¤§è§„æ¨¡åˆæˆæ•°æ®é›†JEDIâ€‹ (400w) å’Œç²¾æ ‡çš„åŸºå‡†æµ‹è¯•OSWORLD-Gâ€‹ (564)  
> é€šè¿‡UIæ„é€ groudingæ•°æ®ï¼Œå¹¶å¤„ç†æˆ(å›¾åƒ+æŒ‡ä»¤â†’åŠ¨ä½œ)çš„æ ¼å¼  
> é€šè¿‡æŒ‡ä»¤-æˆªå›¾é”™é…ç”Ÿæˆä¸å¯è¡ŒåŠ¨ä½œæ ·æœ¬ï¼Œå¢å¼ºæ¨¡å‹æ‹’ç»èƒ½åŠ›
> ç”¨JEDI-7Bä½œä¸ºgroudingæ¨¡å‹ï¼Œé…åˆGPT-4oè§„åˆ’å™¨ï¼Œåœ¨OSWorldåŸºå‡†ä¸Šçš„æˆåŠŸç‡é«˜è¾¾27%

[2505][ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/pdf/2505.23762)
> å¤ç”¨ç”¨osworldçš„task configï¼Œç”¨LLMç”Ÿæˆæ›´å¤šæŒ‡ä»¤
> task verifierç”¨qwen2.5vl-32b


ğŸŒŸ [2501] [UI-TARS: Pioneering Automated GUI Interaction with Native Agents](https://arxiv.org/pdf/2501.12326)
> groundingæ•°æ®ä»å…¬å¼€æ•°æ®é›†ä¸­æ”¶é›†  
> è½¨è¿¹æ•°æ®ä¸¤éƒ¨åˆ†ï¼š145kç§»åŠ¨ç«¯è½¨è¿¹ï¼Œå›¢é˜Ÿæ ‡æ³¨çš„PCè½¨è¿¹  
> ç”¨VLMç»™è½¨è¿¹æ•°æ®æ‰“ä¸Šæ€ç»´é“¾  
> è¿­ä»£ã€è®­ç»ƒ-æ¨è½¨è¿¹-æ ¡æ­£è½¨è¿¹ã€‘è¿™ä¸ªè¿‡ç¨‹  
> æ ¡æ­£è½¨è¿¹é€šè¿‡å¤šçº§è¿‡æ»¤å®ç°ï¼š1ï¼‰è§„åˆ™å¯å‘å¼ï¼šç§»é™¤æ— æ•ˆåŠ¨ä½œï¼›2ï¼‰VLMè¯„åˆ†ï¼šè¿‡æ»¤ä½åˆ†è½¨è¿¹ï¼›3ï¼‰äººå·¥å®¡æ ¸ï¼šæˆªæ–­é”™è¯¯æ­¥éª¤ï¼Œä¿ç•™æœ‰æ•ˆå‰ç¼€  
> ä¸¤ç§dpoè½¨è¿¹å¯¹ï¼š1ï¼‰å…¬å¼6ï¼Œé”™è¯¯æ­¥éª¤-æ­£ç¡®æ­¥éª¤ï¼›2ï¼‰å…¬å¼7ï¼Œé”™è¯¯å‘ç”Ÿåï¼Œä¸é‡‡å–è¡¥æ•‘-é‡‡å–è¡¥æ•‘

### 3.2 Browser
[2507] [WebShaper: Agentically Data Synthesizing via
Information-Seeking Formalization](https://arxiv.org/pdf/2507.15061)
> formulation-driven data synthï¼Œå°†questionç”¨é›†åˆäº¤å¹¶çš„è¯­è¨€å½¢å¼åŒ–  
> åŸºäºå½¢å¼åŒ–æè¿°ï¼Œåˆ©ç”¨expander agentæ‹“å±•seed question

[2504] [Breaking the Data Barrier â€“ Building GUI Agents Through Task Generalization](https://arxiv.org/pdf/2504.10127)
> é’ˆå¯¹Qwen2-VL-7B-Instructï¼Œæå‡ºåœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´å¼•å…¥ä¸€ä¸ªä¸­é—´è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨11ç§ä¸åŒä»»åŠ¡çš„æ•°æ®é›†ï¼ˆåŒ…æ‹¬å¤šæ¨¡æ€å’Œçº¯æ–‡æœ¬ä»»åŠ¡ï¼‰ï¼Œä»¥æå‡æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›ï¼ˆå¦‚æ¨ç†ã€æ„ŸçŸ¥å’Œè§„åˆ’ï¼‰ï¼Œå¯¹GUIä»»åŠ¡å¸®åŠ©å¾ˆå¤§  
> çº¯æ–‡æœ¬æ•°å­¦æ•°æ®ï¼ˆMathInstructï¼‰åœ¨WebArenaåŸºå‡†ä¸Šæå‡5.6%ï¼Œåœ¨AndroidWorldä¸Šæå‡5.4%  
> å¤šæ¨¡æ€æ•°å­¦æ•°æ®æå‡AndroidWorldæ€§èƒ½6.3%

[2502] [InSTA: Towards Internet-Scale Training For Agents](https://arxiv.org/abs/2502.06776)
> è‡ªåŠ¨åŒ–ä¸‰çº§æ•°æ®æµæ°´çº¿: LLMä»»åŠ¡ç”Ÿæˆå™¨ã€LLMè½¨è¿¹ç”Ÿæˆå™¨â€‹ã€LLMè½¨è¿¹è¿‡æ»¤å™¨  
> åŸºäºæµè§ˆå™¨æ²™ç›’å®ä¾‹â€‹ç”Ÿäº§çœŸå®çš„æ•°æ®


[2412] [Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction](https://aguvis-project.github.io)
> stage-1: åŸºç¡€å®šä½æ•°æ®ï¼ˆ103.6ä¸‡æ¡ï¼‰  
> stage-2: è§„åˆ’æ¨ç†è½¨è¿¹æ•°æ®ï¼ˆ3.5ä¸‡æ¡ï¼‰ï¼Œæµè§ˆå™¨6,253ï¼Œç§»åŠ¨ç«¯27,647  
> è½¨è¿¹æ•°æ®ç”±GPT-4oç”Ÿæˆæ€è€ƒé“¾ä¸‰å…ƒç»„(obs, reason, action)

[2412] [ICLR2025] [AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials](https://agenttrek.github.io)
> çˆ¬äº†1880wæ•™ç¨‹ï¼ŒLLMç­›é€‰å‡º23wï¼Œæ•™ç¨‹å–‚ç»™VLMå–æ‰§è¡Œä»»åŠ¡ï¼Œè·å¾—1wè½¨è¿¹
> hfæ•°æ®é›†åº”è¯¥æ˜¯å¯ç”¨çš„ï¼Œè™½ç„¶hfçš„å¯è§†åŒ–å±•ç¤ºæ˜¯å•è½®çš„ï¼Œä½†è®ºæ–‡æåˆ°è½¨è¿¹æ˜¯å¤šè½®çš„ï¼Œä½†ä»¥å•è½®çš„æ ¼å¼å­˜å‚¨

### 3.3 Mobile


## 4. Reward Model
[2508][ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/pdf/2508.14040)
> We manually collect extensive, well-defined tasks and corresponding evaluation functions. é€šè¿‡äººå·¥æ„å»ºä»»åŠ¡ã€éªŒè¯å‡½æ•°  
> If the trajectory successfully solves the task, we assign a reward of 1 to every action that is both correctly formatted and substantially contributes to the solution. éªŒè¯å‡½æ•°åº”è¯¥æ˜¯å¤æ‚åˆ°æ¯ä¸ªactionæ˜¯å¦å¯¹ä»»åŠ¡å®Œæˆæœ‰ç›Šã€‚

[2506][Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/pdf/2506.21252)
> è¦†ç›–agentä»»åŠ¡å¤ªå¤šæ ·ï¼šç½‘é¡µå¯¼èˆªï¼ˆ417ï¼‰â€‹ã€å…·èº«æ™ºèƒ½ï¼ˆ317ï¼‰ã€æ—…è¡Œè§„åˆ’ï¼ˆ180ï¼‰

[2505][ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/pdf/2505.23762)
> task verifierç”¨qwen2.5vl-32b

ğŸŒŸ [2505] [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/pdf/2505.21496)
> å¤šç²’åº¦å¥–åŠ±æ¨¡å‹â€‹ï¼šåŒæ—¶æ”¯æŒstep-levelå’Œoutcome-levelè¯„ä¼°
> æ ¸å¿ƒè¦ç‚¹åœ¨äºå¦‚ä½•æåˆ°æ•°æ®æ¥è®­ç»ƒreward model

[2504] [AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories](https://arxiv.org/abs/2504.08942)
> åœ¨ä¸åŒæµè§ˆå™¨benchmarkWebArena/VisualWebArena/AssistantBench/WorkArena/WorkArena++ä¸Šæ¨äº†GPT-4o/Claude 3.7S/Qwen2.5-VL/Llama 3.3çš„è½¨è¿¹1392æ¡ï¼Œäººå·¥ç²¾å¿ƒæ ‡æ³¨ä»»åŠ¡æˆåŠŸã€å‰¯ä½œç”¨ã€é‡å¤è¡Œä¸º  
> åŸºäºè¿™äº›GTï¼Œåˆ†æç°æœ‰benchmarkçš„è¯„ä¼°çš„å‡†ç¡®ç‡ï¼Œæ­ç¤ºè§„åˆ™è¯„ä¼°çš„ä¸¥é‡ç¼ºé™·ï¼Œå’ŒLLMè¯„ä¼°çš„ç“¶é¢ˆ  
> æå‡ºæ–°å‹ç®€åŒ–è¯„ä¼°æ¡†æ¶ï¼Œå°±æ˜¯æ”¹äº†prompt

## 5. Training Algorithm

| Model | Score | CT | SFT | DPO | RFT | RLVR | Max Steps |
|-------|-------|----|----|-----|-----|------|-----------|
| uitars-2 | 47.5 | âœ… | âœ… | | âœ… | âœ… | âˆ |
| autoglm-os-9b | 47.3 | | | | âœ… | | |
| seagent-7b | 34.5 | | âœ… | âœ… | | | |
| opencua-32b/7b | 34.1/28.2 | | âœ… | | | | 50 |
| sea-7b | 30.1 | | | | âœ… | | |
| gui-owl-7b | 29.4 | | âœ… | | | | 50 |
| uitars-72b-dpo | 25.8 | | âœ… | âœ… | | | 50 |

ğŸŒŸ [2509][UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning](https://arxiv.org/pdf/2509.02544)
**è®­ç»ƒæµç¨‹ï¼š**
- **CT (Curriculum Training)**: ä»»åŠ¡æ•™ç¨‹ã€æ•™å­¦è§†é¢‘ã€in-situ annotation
- **SFT (Supervised Fine-Tuning)**: interactive annotation
- **RLVR (Reinforcement Learning with Verifier Reward)**: åŸºäºéªŒè¯å™¨å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ 

**ä¸‰ç±»è®­ç»ƒä»»åŠ¡ï¼š**
- **GUI-Browsing**: Deep researchä»»åŠ¡ï¼Œä½†ä¸èƒ½è°ƒç”¨æœç´¢APIï¼Œåªèƒ½æ“ä½œç½‘é¡µ
- **GUI-General**: å¤šæ ·åŒ–çš„ç½‘ç«™æ“ä½œä»»åŠ¡
- **Gameplay**: æµè§ˆå™¨ä¸­çš„å°æ¸¸æˆï¼Œåˆæˆæ¸¸æˆ

**è®­ç»ƒç®—æ³•ï¼š** PPO (Proximal Policy Optimization)

**å¥–åŠ±è®¾è®¡ï¼š**
- **Gameplay**: ä½¿ç”¨functionéªŒè¯
- **GUI-Browsing**: ä½¿ç”¨LLMæ ¹æ®GTåŒ¹é…
- **GUI-General**: ä½¿ç”¨UI-TARS-2ä½œä¸ºORM (Outcome Reward Model)

**æ¨¡å‹èåˆç­–ç•¥ï¼š** åŒä¸€ä¸ªSFTæ¨¡å‹åˆ†åˆ«ç”¨ä¸‰ç±»ä»»åŠ¡è¿›è¡ŒRLè®­ç»ƒï¼Œç„¶åè¿›è¡Œå‚æ•°å¹³å‡ (params avg)

ğŸŒŸ [2508][Mobile-Agent-v3: Foundamental Agents for GUI Automation](https://arxiv.org/pdf/2508.15144)
> é¢„è®­ç»ƒ+sftå¾—åˆ°gui-owl-7båœ¨osworldä¸Š29.4ï¼›åœ¨osworldä¸Šä¸“é—¨rlåæ˜¯34.9  

- **Pre-training Phaseï¼ˆé¢„è®­ç»ƒé˜¶æ®µï¼‰**ï¼š
  - æ¶µç›–åŸºç¡€ UI ç†è§£ã€äº¤äº’è½¨è¿¹ã€é€šç”¨æ¨ç†çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­æ–™
  - æŒç»­é¢„è®­ç»ƒ Qwen2.5-VLï¼Œå¼ºåŒ– GUI å…ƒç´ è¯†åˆ«ã€åŠ¨ä½œé¢„æµ‹ã€é€šç”¨æ¨ç†ç­‰åŸºç¡€èƒ½åŠ›
- **Iterative Tuning Phaseï¼ˆè¿­ä»£è°ƒä¼˜é˜¶æ®µï¼‰**ï¼š
  - åœ¨æ¡Œé¢ã€ç§»åŠ¨ç­‰çœŸå®ç¯å¢ƒä¸­éƒ¨ç½²æ¨¡å‹æ‰§è¡Œå¤§è§„æ¨¡ä»»åŠ¡
  - å°†è½¨è¿¹æ¸…æ´—ã€è¯„åˆ†åè½¬æ¢ä¸ºå¤šæ ·åŒ–æ¨ç†æ•°æ®é›†


[2508][ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/pdf/2508.14040)
> We manually collect extensive, well-defined tasks and corresponding evaluation functions.  
> é€šè¿‡äººå·¥æ„å»ºä»»åŠ¡ã€éªŒè¯å‡½æ•°  

> If the trajectory successfully solves the task, we assign a reward of 1 to every action that is both correctly formatted and substantially contributes to the solution.  
> éªŒè¯å‡½æ•°åº”è¯¥æ˜¯å¤æ‚åˆ°æ¯ä¸ªactionæ˜¯å¦å¯¹ä»»åŠ¡å®Œæˆæœ‰ç›Šã€‚ 

**SFT**ï¼š
  - Model poolæä¾›è½¨è¿¹æ•°æ®

**RLVR**ï¼š
  - **Step-Level Group Relative Policy Optimization**ï¼š
    - ä¸€ä¸ªqueryçš„æ‰€æœ‰rolloutçš„æ‰€æœ‰stepä½œä¸ºä¸€ä¸ªgroup
    - æ¯ä¸ªstepè®¡ç®—rewardï¼Œè¿›è€Œè®¡ç®—advantage
  - **å¥–åŠ±è®¾è®¡**ï¼š
    - è‹¥è½¨è¿¹æˆåŠŸå®Œæˆä»»åŠ¡ï¼Œåˆ™ä¸ºæ‰€æœ‰æ ¼å¼æ­£ç¡®ä¸”å¯¹è§£å†³æ–¹æ¡ˆæœ‰å®è´¨è´¡çŒ®çš„åŠ¨ä½œåˆ†é…1åˆ†å¥–åŠ±
    - å¦åˆ™ï¼Œæœªé€šè¿‡éªŒè¯çš„è½¨è¿¹æˆ–æ ¼å¼ä¸å½“çš„åŠ¨ä½œå‡å¾—0åˆ†
  - **Entropulseç­–ç•¥**ï¼š
    - æ”¶é›†rolloutï¼Œç­›é€‰æˆåŠŸçš„
    - RLä¹‹é—´ç©¿æ’SFTè®­ç»ƒè¿™äº›ï¼Œå¯ä»¥æé«˜entropyã€çªç ´æ€§èƒ½ç“¶é¢ˆ


[2508][SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/pdf/2508.04037)
> 7bæ¨¡å‹åœ¨osworldä¸Š30.1ï¼Œæ¯”SE-Agentã€OpenCUAéƒ½æ›´å¼º  
> åŸºäºQWen2.5-VL-72Bè®­ç»ƒå‡ºæ¥çš„step modelå¯ä»¥åˆ¤æ–­actionçš„æ­£ç¡®æ€§ï¼Œä½†æ²¡å±•ç¤ºprompt  
> é€šè¿‡few-shot in-context Learningå»ç”Ÿäº§task

ğŸŒŸ [2508] [SEAgent: Self-Evolving Computer Use Agent with
Autonomous Learning from Experience](https://arxiv.org/pdf/2508.04700)
> Curriculum Generator: ç”Ÿæˆä»»åŠ¡ï¼Œæäº†4245ä¸ªä»»åŠ¡  
> World State Model: ä¹Ÿå°±æ˜¯reward model, ç”¨çš„æ˜¯åŸºäºosworld chomeçš„43ä¸ªtaskå¾—åˆ°çš„860æ¡è½¨è¿¹è®­ç»ƒqwen2.5vl-7bå¾—åˆ°çš„

**SEAgentæ¡†æ¶ï¼š**
1. **Task Initializationï¼ˆä»»åŠ¡åˆå§‹åŒ–ï¼‰**
2. **Autonomous Evaluationï¼ˆè‡ªä¸»è¯„ä¼°ï¼‰**
3. **RFTï¼ˆRejection Fine-Tuningï¼‰**
4. **Task Updateï¼ˆä»»åŠ¡æ›´æ–°ï¼‰**

**RFTæŠ€æœ¯ç»†èŠ‚ï¼š**
- **é”™è¯¯åŠ¨ä½œæƒ©ç½š**ï¼šå¯¹è¯†åˆ«å‡ºçš„é”™è¯¯åŠ¨ä½œè¿›è¡Œæƒ©ç½šæ€§è®­ç»ƒ
- **æ­£ç¡®åŠ¨ä½œæ¨¡ä»¿**ï¼šå­¦ä¹ å’Œæ¨¡ä»¿æ­£ç¡®çš„æ“ä½œåºåˆ—



ğŸŒŸ [2508] [OpenCUA: Open Foundations for Computer-Use Agents](https://opencua.xlang.ai)
> 634åæ ‡æ³¨å‘˜é’ˆå¯¹200+åº”ç”¨/ç½‘ç«™ï¼Œæ ‡è®°äº†20k+ä»»åŠ¡çš„è½¨è¿¹  
> å¼€æºäº†æ ‡æ³¨è½¯ä»¶ã€æ•°æ®é›†  
> çº¯sftè®­ç»ƒqwen-vl-32bï¼Œosworldä¸Šåˆ†æ•°34.8

**è½¨è¿¹æ•°æ®ç‰¹æ€§ï¼š**
- **æ•°æ®è§„æ¨¡**ï¼šåŒ…å«22,625æ¡äººå·¥æ ‡æ³¨çš„è®¡ç®—æœºä½¿ç”¨ä»»åŠ¡
- **å¹³å°åˆ†å¸ƒ**ï¼šWindowså¹³å°12Kæ¡ã€macOSå¹³å°5Kæ¡ã€Ubuntuå¹³å°5Kæ¡
- **æŠ€æœ¯è§„æ ¼**ï¼šå±å¹•åˆ†è¾¨ç‡æ¶µç›–720pè‡³4K
- **å¤æ‚åº¦**ï¼šæ¯æ¡è½¨è¿¹å¹³å‡18.6æ­¥ï¼Œä½“ç°äº†ä»»åŠ¡çš„å¤æ‚æ€§
- **è¦†ç›–èŒƒå›´**ï¼šæ•°æ®è¦†ç›–140å¤šä¸ªåº”ç”¨ç¨‹åºå’Œ190å¤šä¸ªç½‘ç«™ï¼Œå¸¸æ¶‰åŠå¤šåº”ç”¨å·¥ä½œæµã€ä¸“ä¸šå·¥å…·å’Œä¸å¸¸ç”¨åŠŸèƒ½
- **æ•°æ®é›†ç‰¹è‰²**ï¼šä¸ç°æœ‰GUIæ•°æ®é›†ç›¸æ¯”ï¼ŒAGENTNETæ˜¯é¦–ä¸ªå…¼å…·çœŸå®æ€§ã€å¤æ‚æ€§ã€å¤šæ ·æ€§å’Œå¤šæ¨¡æ€ç‰¹æ€§çš„æ¡Œé¢è½¨è¿¹çº§æ•°æ®é›†

**è®­ç»ƒæ•°æ®æ··åˆï¼š**
- **å¤šå±‚çº§è½¨è¿¹COT**ï¼šæ··åˆä¸åŒå±‚çº§çš„è½¨è¿¹æ€ç»´é“¾
  - L1ï¼ˆåŠ¨ä½œï¼‰
  - L2ï¼ˆæ€è€ƒ + åŠ¨ä½œï¼‰
  - L3ï¼ˆè§‚å¯Ÿ + æ€è€ƒ + åŠ¨ä½œï¼‰
- **æ•°æ®ç»„æˆ**ï¼šè½¨è¿¹æ•°æ®ã€groundingæ•°æ®ã€é€šç”¨SFTæ•°æ®

**è®­ç»ƒæµç¨‹ï¼š**
- **ä¸‰ç§è®­ç»ƒç­–ç•¥**ï¼š
  - **ä»…é˜¶æ®µ2**ï¼šopencua-qwen2-7bå’Œopencua-a3b
  - **é˜¶æ®µ1+é˜¶æ®µ2**ï¼šopencua-32b
  - **è”åˆç­–ç•¥**ï¼šopencua-7b

- **å…·ä½“é…æ¯”**ï¼š
  - **ä»…é˜¶æ®µ2**ï¼š56%è½¨è¿¹æ•°æ®ã€14%groundingæ•°æ®ã€30%é€šç”¨SFTæ•°æ®
  - **é˜¶æ®µ1+é˜¶æ®µ2**ï¼š
    - é˜¶æ®µ1ï¼šgroundingæ•°æ®ã€æ•™ç¨‹å¼æ¼”ç¤ºã€çŠ¶æ€è½¬æ¢æè¿°æ•°æ®ã€é€šç”¨VLå’Œé€šç”¨text SFTæ•°æ®
    - é˜¶æ®µ2ï¼š45%è½¨è¿¹æ•°æ®ã€20%groundingæ•°æ®ã€35%é€šç”¨æ•°æ®
  - **è”åˆç­–ç•¥**ï¼š20%è½¨è¿¹æ•°æ®ã€20%groundingæ•°æ®ã€60%é€šç”¨æ•°æ®


[2505][ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/pdf/2505.23762)
> å¤ç”¨ç”¨osworldçš„task configï¼Œç”¨LLMç”Ÿæˆæ›´å¤šæŒ‡ä»¤
> task verifierç”¨qwen2.5vl-32b

[2504] [UI-TARS-1.5](https://seed-tars.com/1.5/)
> æ²¡æœ‰æŠ«éœ²å¤ªå¤šç»†èŠ‚ï¼Œåªæåˆ°äº†â€œUI-TARS-1.5 integrates advanced reasoning enabled by reinforcement learningâ€  
> osworldä¸Šåˆ†æ•°42.5ï¼Œ7bçš„åˆ™æ˜¯28

ğŸŒŸ [2501] [UI-TARS: Pioneering Automated GUI Interaction with Native Agents](https://arxiv.org/pdf/2501.12326)
> groundingæ•°æ®ä»å…¬å¼€æ•°æ®é›†ä¸­æ”¶é›†  
> è½¨è¿¹æ•°æ®ä¸¤éƒ¨åˆ†ï¼š145kç§»åŠ¨ç«¯è½¨è¿¹ï¼Œå›¢é˜Ÿæ ‡æ³¨çš„PCè½¨è¿¹  
> ç”¨VLMç»™è½¨è¿¹æ•°æ®æ‰“ä¸Šæ€ç»´é“¾  
> è¿­ä»£ã€è®­ç»ƒ-æ¨è½¨è¿¹-æ ¡æ­£è½¨è¿¹ã€‘è¿™ä¸ªè¿‡ç¨‹  
> æ ¡æ­£è½¨è¿¹é€šè¿‡å¤šçº§è¿‡æ»¤å®ç°ï¼š1ï¼‰è§„åˆ™å¯å‘å¼ï¼šç§»é™¤æ— æ•ˆåŠ¨ä½œï¼›2ï¼‰VLMè¯„åˆ†ï¼šè¿‡æ»¤ä½åˆ†è½¨è¿¹ï¼›3ï¼‰äººå·¥å®¡æ ¸ï¼šæˆªæ–­é”™è¯¯æ­¥éª¤ï¼Œä¿ç•™æœ‰æ•ˆå‰ç¼€  
> ä¸¤ç§dpoè½¨è¿¹å¯¹ï¼š1ï¼‰å…¬å¼6ï¼Œé”™è¯¯æ­¥éª¤-æ­£ç¡®æ­¥éª¤ï¼›2ï¼‰å…¬å¼7ï¼Œé”™è¯¯å‘ç”Ÿåï¼Œä¸é‡‡å–è¡¥æ•‘-é‡‡å–è¡¥æ•‘

## 6. Others
### x.1 General Agent
[2507] [Kimi K2: Open Agentic Intelligence](https://moonshotai.github.io/Kimi-K2/)

[2507] [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/pdf/2507.01006)
> osworldä¸Šåˆ†æ•°ä¸å¤ªå¥½åªæœ‰14.7ï¼Œä½†å¯èƒ½æ˜¯æ²¡æœ‰ä¸“é¡¹å¾®è°ƒçš„åŸå› ï¼Œæ½œåŠ›åº”è¯¥æŒºå¤§

### x.2 Coding
[Qwen3-Coder: Agentic Coding in the World](https://qwenlm.github.io/blog/qwen3-coder/)

### x.3 Math


### x.4 Benchmark
ğŸŒŸ [2507] [Establishing Best Practices for Building Rigorous
Agentic Benchmarks](https://arxiv.org/pdf/2507.02825)
> æŒ‡å‡ºäº†ç°æœ‰agent benchmarkçš„é—®é¢˜ï¼Œä¾‹å¦‚webarenaå°±æœ‰é—®é¢˜  
> æå‡ºäº†ABCæ£€æŸ¥è¡¨ï¼Œä½œä¸ºæ£€æŸ¥ã€æ„å»ºagent benchmarkçš„åŸåˆ™  
> ä½œä¸ºä¸€ç§meta-researchï¼Œæ¯”è¾ƒæœ‰å¯å‘æ€§

[2507] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/pdf/2507.05495)
> é’ˆå¯¹deep researchçš„arenaï¼Œäº®ç‚¹æ˜¯é™¤äº†æœ€ç»ˆç»“æœï¼Œè¿˜å¯ä»¥å¯¹ä¸­é—´æ­¥éª¤ç‚¹èµ

[2501] [ACEBench: Who Wins the Match Point in Tool Usage?](https://arxiv.org/pdf/2501.12851)